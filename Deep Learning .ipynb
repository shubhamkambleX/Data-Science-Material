{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning is basically used to solved complex problems of real world\n",
    "# what is neural network\n",
    "# in this it uses input layer from which the multiple inputs are provided and that input layer contains\n",
    "# some weight.\n",
    "# and that input layer is pass to hidden layer for perform some operations and then generate \n",
    "# the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN(Artifical Neural Network)\n",
    "# in this the many signal are given as input with there particular wight i.e (1 to nth number of signal/input)\n",
    "# this input are provide to the node in which two seperate work is done\n",
    "# 1) sumation(E): in this sumation of (1stinput*1stweight) + nth(input) * nth(weight)) is done for many input as given.\n",
    "# 2) function(Activation function): y=f(E xi*wi) \n",
    "# there are 3 typs of activation function:\n",
    "#     1) Linear Activation function\n",
    "#     2) Heviside Activation Function\n",
    "#     3) sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Neural Network Training \n",
    "# the input are pass to hidden layer and to generate the output.\n",
    "# when the predicted output is different from the actual output then optimizer is used to reduce the\n",
    "# loss\n",
    "# formula for loass function:\n",
    "#     loss = (y_original-y_predict)^2\n",
    "        # or\n",
    "# activation function is used.\n",
    "# as the error raises the back propagation is done while doing this we have to update the weight\n",
    "# and used for minimum error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how weights are updated \n",
    "# by using the back propagation and optimaizer\n",
    "# # gradient decent is one of the method of optimiazer\n",
    "# formula for weight updation :\n",
    "#     w_new= w_old-n(df(loss)/df(weight))    where n= learning rate or we can take alpha also\n",
    "\n",
    "# steps for that \n",
    "# take the output node from which error has generated\n",
    "# the node contains the output of previous weight and x value which is called net value\n",
    "# then for node value updation \n",
    "# formula is used by chain rule \n",
    "# dEtotal/d(finalweight) = (dEtotal/d(output1)) * (d(output1)/d(net1)) * (d(net1)/d(finalweight))\n",
    "# (dEtotal/d(output1)) = (out1 - target1)\n",
    "# (d(output1)/d(net1)) =  (out1(1-out1))\n",
    "# (d(net1)/d(finalweight)) = finalnodeouput   take that value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanishing gradient problem\n",
    "# basically it is because of sigmaoid function\n",
    "# as the sigmoid function it used each and every node,so sigmoid function ranges from 0 to 1 \n",
    "# and d(sidmoid) ranges from 0 to 0.25 as the number of hidden layer increase the derivate may decrease\n",
    "# and it (gradent value)reduces the value to one particular level that the new weight and old weight matches\n",
    "# therfore vanishing gradent problem come into picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploed the grandent problem\n",
    "# this issue occurs when the weight is initialized is high as the d(sigmoid) ranges between 0 to 0.25\n",
    "# the d(eTotal/output1) may be vary from negative value \n",
    "# therfore the exploed can be happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu(Rectified Linear unit )\n",
    "# this activation function has linear line\n",
    "# which range from max(0,z), this can solve the problem of sigmoid and tanh but \n",
    "# when the value we take 0 of then derivate be 0 in this case it may create dead neuron.\n",
    "# this can be solve by leaky relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaky relu:\n",
    "# this activation function has linear line and ranges from max(0.01x,x) ,this can solve problem of relu\n",
    "# as it can generate negative values,\n",
    "# so this can solve the dead neuron problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elu(exponential linear unit):\n",
    "# this activation function f(x,aplha(e^x-1), if x>0)\n",
    "# so in this no problem of dead neuron and it is better than relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight inilization techniques \n",
    "# the weight should not be minimum\n",
    "# the weight should not be mamximum\n",
    "# the weight should contain middle value between min to max\n",
    "\n",
    "# 1) uniform distribution:\n",
    "# in this weights are selected from [-1/rt[fan-in],1/rt[fan-in]]   fan-in = input/ fan-out= output\n",
    "# 2) xavior/gorat distribution:\n",
    "# it consist of 2 types:\n",
    "#     1) xaviour normal\n",
    "#     2)gorat normal\n",
    "# 3)he init:\n",
    "# 1) he uniform\n",
    "# 2) he normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All optimizers: all optimizers basically used to reduse the loss of and update the weight\n",
    "# by back propagation\n",
    "# there are many optimizers such as:\n",
    "#     1) Gradint Decent:\n",
    "# gradient decent : it consist of n datapoints i.e( complete dataset is given for weight \n",
    "# updation with back propagation)\n",
    "# there was problem where more data can take more time to update the weight\n",
    "# as computational problem could arises\n",
    "# formula: i = 1 to n E(y-y_hat)^2\n",
    "#     # 2)SGD(stochtic gradint decent): it consist of 1 particular datapoint while updating the weight\n",
    "# with back propagation\n",
    "# there was problem where 1datapoint were executing therefore data can take more time to update the weight\n",
    "# formula: (y-y_hat)^2\n",
    "#     3) Mini Batch SGD:\n",
    "# mini batch SGD: it consist of k datapoints means it generate the epoch \n",
    "# i.e(number of (iteration = total/batchsize))\n",
    "# with forword and backword propagation\n",
    "# as it contains the batch therefore it may execute fast and not taking more time \n",
    "# but it create the noise as the whole dataset is not provided and batch is taken\n",
    "# formula : i=1 to k E(y-y_hat)^2\n",
    "#    4) SGD with momentom:\n",
    "# in this while reaching to global minima \n",
    "# it genarally create noise ,so to reduce the noise we use momentom\n",
    "# formula: as Y = 0.9 approximetly taken\n",
    "#     Wnew = Wold - Lrate * [d[loss]/d[Wold]]\n",
    "#     Wnew = Wold - [YVt-1] + Lrate * [d[loss]/d[Wold]] where [YVt-1] = momemtom\n",
    "# so it can be written as \n",
    "# Vt-1 = 1*[d[loss]/d[Wold]]t + Y[d[loss]/d[Wold]]t-1 ..... Y^n[d[loss]/d[Wold]]t-n\n",
    "\n",
    "#   5) Adagrad Optimizer (Adaptive Gradient optimizer)\n",
    "# this optimizer is used for change in learning rate as not keeping same\n",
    "# formula: Lrate = n/rt(<t + E)\n",
    "# as the number of iteration increase the Lrate will also increare so the it may create \n",
    "# problem <t = [d(Loss)/d(w)]^2\n",
    "#     6)Adadelta and RMSProp:it is same like adagrad but in this the Lrate value is decreace\n",
    "# as the exponential weight avg is taken small value\n",
    "# so <t is also written as Sd(w)\n",
    "# were Sd(w)= BSd(w)t + (1-B)*[d(Loss)/d(w)^2] \n",
    "# were (1-B) value is so small therfore it reduses the Lambda value\n",
    "#     7)Adam optimizer(Adaptive moment estimation):\n",
    "# they both have two things \n",
    "# momentum: it brings smothness\n",
    "# RMSProp: it changes the Lrate \n",
    "#     it is best optimizer and common used by all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
